# LLM_Reasoning_Personality_Knowledge_Graph_Challenge_Onyekachukwu_Ekesi
The Intellumia Internship Challenge

# ğŸ§  LLM Reasoning & Personality Knowledge Graph Challenge

This repository contains my solution to the **LLM Reasoning & Personality Knowledge Graph Challenge**, a Data Science internship assessment focused on evaluating research capability, reasoning with LLMs, and practical implementation of knowledge graph concepts.

---

## ğŸš€ Project Overview

The goal of this challenge is to **develop a Python solution** that:
1. Takes a **text document** as input,
2. **Extracts entities and relationships** to build a **Knowledge Graph (KG)**, and
3. **Models personality traits** of the subjects within the graph.

This open-ended problem emphasizes **logical reasoning**, **independent research**, and **effective LLM utilization** â€” not just code writing.  
Hence, every major design decision, experiment, and refinement step was guided by LLM-assisted reasoning.

---

## ğŸ“ Repository Structure

```bash
llm-kg-personality/
â”œâ”€ data/
â”‚ â”œâ”€ synthetic/ # Generated sample documents + ground-truth triples & traits
â”‚ â”œâ”€ output/ # Saved graph outputs (GraphML / JSON)
â”‚
â”œâ”€ notebooks/
â”‚ â”œâ”€ 01_pipeline.ipynb # Full end-to-end demonstration notebook
â”‚
â”œâ”€ src/
â”‚ â”œâ”€ preprocessing.py # Text preprocessing and sentence segmentation
â”‚ â”œâ”€ extractors.py # Entity and relation extraction
â”‚ â”œâ”€ llm_prompts.py # LLM prompt templates & call functions
â”‚ â”œâ”€ kg_builder.py # Knowledge Graph construction
â”‚ â”œâ”€ synthetic_generator.py # Synthetic data generation with personality traits
â”‚ â”œâ”€ evaluate.py # Evaluation metrics for triples & traits
â”‚
â”œâ”€ LLM_SESSION/ # Saved LLM session screenshots / exports
â”‚ â”œâ”€ session1.txt
â”‚ â”œâ”€ session2.txt
â”‚
â”œâ”€ REPORT.md # Final short report (generated using LLM)
â”œâ”€ README.md # This documentation file
â”œâ”€ requirements.txt # Python dependencies
```

---

## ğŸ§© Approach Summary

The pipeline follows a hybrid **rule-based + LLM-guided** approach:

1. **Preprocessing** â€“ Tokenize, segment, and extract entities using `spaCy`.
2. **Candidate Triple Extraction** â€“ Use dependency patterns to form preliminary triples `(subject, predicate, object)`.
3. **LLM Refinement** â€“ Ask an LLM to:
   - Validate triples
   - Canonicalize relations
   - Infer missing relationships
   - Derive personality traits (Big Five model)
4. **Knowledge Graph Construction** â€“ Build a directed graph using `NetworkX`, linking entities and traits.
5. **Evaluation** â€“ Compare against synthetic ground-truth using F1 for triples and RMSE for traits.

---

## ğŸ§  Personality Modelling

Personality traits are represented using the **Big Five (OCEAN)** model:
- **Openness**
- **Conscientiousness**
- **Extraversion**
- **Agreeableness**
- **Neuroticism**

Each person node is connected to trait nodes through `"has_trait"` edges, where trait nodes carry numeric scores (0â€“1).

Example representation:
```bash
Person: Alice
â”œâ”€â”€ has_trait â†’ openness (0.82)
â”œâ”€â”€ has_trait â†’ conscientiousness (0.90)
â””â”€â”€ has_trait â†’ extraversion (0.65)
```

---

## ğŸ§¬ Synthetic Data Generation

Since no real data is provided, synthetic biographies were generated programmatically.  
Each document contains short biographical sentences describing:
- Profession
- Habits / behaviors
- Interests
- Emotional tendencies

Ground-truth **triples** and **trait scores** were created alongside to enable evaluation.

Example:
```json
{
  "text": "Alice is a careful and organised project manager who enjoys reading speculative fiction.",
  "triples": [
    ["Alice", "occupation", "project manager"],
    ["Alice", "hobby", "reading"]
  ],
  "traits": {
    "openness": 0.77,
    "conscientiousness": 0.92,
    "extraversion": 0.54,
    "agreeableness": 0.81,
    "neuroticism": 0.28
  }
}
```

## ğŸ”„ Workflow Summary
### 1. Generate Synthetic Data

```bash
Generate Synthetic Data
python src/synthetic_generator.py
```
### 2. Run Pipeline (Extraction + LLM Refinement + KG Building)

```bash
python src/synthetic_generator.py
```
### 3. Evaluate Model

```bash
python src/evaluate.py
```
### 4. Visualize Knowledge Graph
Inside the notebook:

```bash
import networkx as nx
nx.draw(G, with_labels=True)
```
## ğŸ“Š Evaluation Metrics

| Task                      | Metric                | Description                                       |
| ------------------------- | --------------------- | ------------------------------------------------- |
| **Triple Extraction**     | Precision, Recall, F1 | Match predicted triples against ground truth      |
| **Personality Inference** | RMSE                  | Compare predicted trait scores to ground truth    |
| **Qualitative Analysis**  | Manual Review         | Sanity-check correctness and consistency of graph |

## ğŸ§° Tools & Libraries

| Category               | Tools                          |
| ---------------------- | ------------------------------ |
| **Core NLP**           | `spaCy`, `transformers`        |
| **Graph Construction** | `NetworkX`                     |
| **Evaluation**         | `scikit-learn`, `pandas`       |
| **LLM Assistance**     | ChatGPT / OpenAI API           |
| **Environment**        | Python â‰¥ 3.9, Jupyter Notebook |


## ğŸ’¬ LLM Workflow

- The pipeline uses chain-of-prompts for modular reasoning:
- Entity canonicalization
- Triple validation & enrichment
- Personality inference
- Contradiction checks
- Graph building instructions

Example prompt:
```vbnet
You are an assistant that extracts factual triples from text.
Sentence: "John is outgoing and often leads teams."
Return factual triples as JSON and infer Big Five personality scores for John.
```

## ğŸ§¾ Deliverables

- âœ… Public GitHub Repository (this repo)
- âœ… Python code & notebook
- âœ… Synthetic dataset
- âœ… Evaluation metrics
- âœ… LLM session exports (/LLM_SESSION/)
- âœ… LLM-generated report (REPORT.md)

## ğŸ§  Insights & Limitations

- Insights
- LLM-guided refinement significantly improved relation consistency.
- Personality inference worked well with descriptive sentences, less so with implicit cues.
- Hybrid rule + LLM approach yields more reliable graphs than pure prompting.
- Limitations
- Personality extraction heavily depends on context length and description richness.
- LLMs can hallucinate non-existent relations.
- Synthetic data limits real-world generalizability.

## ğŸ§© Future Improvements

- Integrate LangChain or LLMGraph to orchestrate multi-step LLM calls.
- Use Neo4j or RDFLib for graph database integration.
- Extend personality modelling beyond Big Five (e.g., MBTI-based reasoning).
- Explore fine-tuning LLMs for domain-specific relation extraction.

## ğŸ§‘â€ğŸ’» Author

**Onyekachukwu Ekesi** 

Data Science & Business Analytics | Machine Learning | Power BI
- ğŸ“§ Email Me: onyekaekesi@gmail.com
- ğŸŒ LinkedIn: https://www.linkedin.com/in/onyekachukwu-ekesi/
- ğŸ’» Portfolio: https://www.datascienceportfol.io/onyekaekesi
